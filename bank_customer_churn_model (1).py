# -*- coding: utf-8 -*-
"""Bank_customer_churn_model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L05DcY0hdkIIzZouJiS7ZzkK3RsL6i-M

**Objectives:**

Customer Retention: The primary objective of a churn prediction model is to help banks retain valuable customers. By identifying customers at risk of churning, the bank can take proactive steps to retain them, such as offering incentives, personalized offers, or improved customer service.

Revenue Protection: Churn can lead to a loss of revenue for the bank. The model helps the bank in preventing this loss by targeting high-value customers who are at risk of churning and taking measures to retain them.

Customized Marketing and Offers: Churn prediction models enable banks to create personalized marketing campaigns and offers for customers who are likely to leave. This can include tailored promotions, loyalty rewards, or other incentives to encourage them to stay.

Risk Management: Identifying customers at risk of churning also helps in risk management. Banks can assess the potential impact of losing specific customers and take actions to mitigate these risks.

Data-Driven Decision Making: Churn prediction models promote data-driven decision-making within the bank. They provide actionable insights that can inform strategies for customer retention and business growth.

**Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Import Data**"""

df = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')
print(df)

"""**Analysis Data**"""

df.head()

df.info()

df.duplicated('CustomerId').sum()

df = df.set_index('CustomerId')

df.info()

df.describe()

"""**Encoding**"""

df['Geography'].value_counts()

df.replace({'Geography':{'France':2,'Germany':1,'Spain':0}},inplace=True)

df['Gender'].value_counts()

df.replace({'Gender':{'Male':0,'Female':1}},inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num Of Products':{1:0,2:1,3:1,4:1}},inplace=True)

df['Has Credit Card'].value_counts()

df['Is Active Member'].value_counts()

df.loc[(df['Balance'] == 0), 'Churn'].value_counts()

df['Zer Balance'] = np.where(df['Balance']>0,1,0)

df['Zer Balance'].hist()

df.groupby(['Churn','Geography']).count()

"""**Define Label and Features**"""

df.columns

x = df.drop(['Surname','Churn'], axis = 1)

y = df['Churn']

df['Churn'].value_counts()

sns.countplot(x = 'Churn', data = df);

x.shape,y.shape

"""**Random Under Sampling**"""

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=2529)

x_rus,y_rus = rus.fit_resample(x,y)

x_rus.shape,y_rus.shape,x.shape,y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind = 'hist')

"""**Random Over Sampling**"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=2529)

x_ros,y_ros = ros.fit_resample(x,y)

x_ros.shape,y_ros.shape,x.shape,y.shape

y.value_counts()

y_ros.value_counts()

y_ros.plot(kind = 'hist')

"""**Train Test Split**"""

from sklearn.model_selection import train_test_split

"""Split Original Data"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state=2529)

"""Split Random Under Sample Data"""

x_train_rus,x_test_rus,y_train_rus,y_test_rus = train_test_split(x_rus,y_rus,test_size = 0.3,random_state=2529)

"""Split Random Over Sample Data"""

x_train_ros,x_test_ros,y_train_ros,y_test_ros = train_test_split(x_ros,y_ros,test_size = 0.3,random_state=2529)

"""**Standard Features**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

"""**Standard Original Data**"""

x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Standardize Random Under Sample Data**"""

x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Standardize Random over Sample Data**"""

x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Support Vector Machine Classifier**"""

from sklearn.svm import SVC
svc = SVC()

svc.fit(x_train,y_train)

y_pred = svc.predict(x_test)

"""**Model Accuracy**"""

from sklearn.metrics import confusion_matrix,classification_report

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

"""**Hyperparameter Tunning**"""

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid = GridSearchCV(SVC(),param_grid,refit = True,verbose = 2, cv = 2)
grid.fit(x_train,y_train)

print(grid.best_estimator_)

grid_predictions = grid.predict(x_test)

confusion_matrix(y_test,grid_predictions)

print(classification_report(y_test,grid_predictions))

"""**Model with Random Under Sampling**"""

svc_rus = SVC()

svc_rus.fit(x_train_rus,y_train_rus)

y_pred_rus = svc_rus.predict(x_test_rus)

"""**Model Accuracy**"""

confusion_matrix(y_test_rus,y_pred_rus)

print(classification_report(y_test_rus,y_pred_rus))

"""**Hyperparameter Tunning**"""

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_rus = GridSearchCV(SVC(),param_grid,refit = True,verbose = 2, cv = 2)
grid_rus.fit(x_train_rus,y_train_rus)

print(grid_rus.best_estimator_)

grid_predictions_rus = grid_rus.predict(x_test_rus)

confusion_matrix(y_test_rus,grid_predictions_rus)

print(classification_report(y_test_rus,grid_predictions_rus))

"""**Model with Random  Over Sampling**"""

svc_ros = SVC()

svc_ros.fit(x_train_ros,y_train_ros)

y_pred_ros = svc_ros.predict(x_test_ros)

"""**Model Accuracy**"""

confusion_matrix(y_test_ros,y_pred_ros)

print(classification_report(y_test_ros,y_pred_ros))

"""**Hyperparameter Tunning**"""

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_ros = GridSearchCV(SVC(),param_grid,refit = True,verbose = 2, cv = 2)
grid_ros.fit(x_train_ros,y_train_ros)

print(grid_ros.best_estimator_)

grid_predictions_ros = grid_ros.predict(x_test_ros)

confusion_matrix(y_test_ros,grid_predictions_ros)

print(classification_report(y_test_ros,grid_predictions_ros))

"""**Let's Compare**"""

print(classification_report(y_test,y_pred))

print(classification_report(y_test,grid_predictions))

print(classification_report(y_test_rus,y_pred_rus))

print(classification_report(y_test_rus,grid_predictions_rus))

print(classification_report(y_test_ros,y_pred_ros))

print(classification_report(y_test_ros,grid_predictions_ros))

"""**Explanation:**

Churn Prediction Model: This model is built using historical customer data, including transaction history, demographics, customer behavior, and interactions with the bank.

Machine Learning Algorithms: Churn prediction models typically use machine learning algorithms such as logistic regression, decision trees, random forests, or neural networks to analyze the data and make predictions.

Features and Variables: The model considers various features or variables like account balance, transaction frequency, customer age, loan history, customer service interactions, etc., to identify patterns and correlations associated with customer churn.

Data Preprocessing: Data preprocessing techniques like data cleaning, feature engineering, and scaling are applied to ensure the data is suitable for modeling.

Training and Testing: The model is trained on historical data, and its performance is evaluated on a separate dataset to ensure it can make accurate predictions on new, unseen data.
"""